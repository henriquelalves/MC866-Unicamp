{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 6400)\n",
      "(200,)\n",
      "[-2.72935972e-02 -2.89927777e-02  2.89913405e-02  6.16473523e-02\n",
      " -8.29663125e-02  5.96450098e-02 -3.49357404e-02  6.12822860e-02\n",
      " -5.98310529e-02  1.27185253e-01  1.41151356e-01  4.34000655e-02\n",
      " -1.33999798e-01 -1.65001533e-02 -5.83991930e-02  1.50654765e-02\n",
      " -6.45794160e-02 -3.93361547e-02 -3.03267826e-02 -6.91507617e-02\n",
      " -5.71932357e-02 -3.11392632e-02  1.90930013e-01 -8.88806150e-02\n",
      " -1.66079824e-01  6.46896903e-02 -6.42126332e-03  2.52101836e-02\n",
      " -2.81147877e-05 -2.05381985e-02  8.00128401e-02  3.98621090e-02\n",
      " -2.51221791e-02  6.74266177e-02 -8.64256452e-02  1.90401614e-02\n",
      " -7.32299362e-02  9.08965704e-03  1.32388916e-01 -1.49546545e-01\n",
      "  3.43227828e-02  4.94505295e-02 -1.63880950e-02  1.34450308e-02\n",
      " -2.86368167e-03 -3.92853520e-03 -5.40890343e-02 -3.30412056e-02\n",
      " -9.12302105e-02 -1.38813589e-01 -1.41936840e-02  1.20829003e-02\n",
      " -1.21272043e-01  5.08899946e-02 -5.55237664e-03 -2.89785324e-02\n",
      " -4.08365415e-02  4.80230376e-02 -4.89752796e-02 -3.78875751e-02\n",
      " -3.80300493e-02  1.28175533e-01  1.35016769e-02  6.50624370e-02\n",
      "  2.95314811e-02  7.28357234e-02 -2.97997797e-02 -1.12403496e-02\n",
      "  2.38522240e-02  9.21372302e-02 -1.04132708e-02 -1.60088605e-02\n",
      "  5.14308404e-03  5.53626645e-02  7.52549739e-02  6.04042891e-02\n",
      "  3.07033619e-02  2.32750879e-02  2.01397523e-01 -1.27973422e-02\n",
      "  1.99777018e-02 -1.16333085e-01 -7.60721393e-02 -1.05701679e-01\n",
      " -2.10009517e-02 -2.51029353e-02 -7.69650949e-03 -4.09214687e-02\n",
      " -1.06395687e-01  5.44907769e-02 -1.17182474e-01  5.60076369e-02\n",
      "  6.42106216e-03  3.43388944e-02  1.00062398e-02  1.91452094e-01\n",
      "  1.48962743e-02  1.90683793e-02  1.14696586e-01  7.05830702e-02\n",
      " -2.78548284e-03 -1.22592475e-01  8.25405430e-03  1.41641800e-01\n",
      "  2.89304381e-02  7.47092114e-02  6.43734349e-02  5.09438797e-02\n",
      "  1.43110603e-02 -8.11565114e-03 -7.63068226e-03 -9.43294292e-02\n",
      " -2.01486521e-02  2.07792111e-02  2.29713420e-02  9.63759529e-02\n",
      "  1.68879333e-03  8.50844273e-04  3.65100355e-03 -2.98074616e-02\n",
      " -7.66685382e-02 -4.92683060e-02 -1.92902567e-02  4.26002893e-02\n",
      "  8.39562902e-02  8.13735868e-03 -1.59497337e-01 -9.93839908e-02\n",
      " -6.43542862e-03 -1.89510551e-02  2.99301656e-03 -4.75015474e-02\n",
      "  7.86005383e-02 -5.88527038e-04  3.86332828e-02 -5.46854799e-02\n",
      " -5.41079060e-02  7.41160630e-02  2.13201647e-02  5.33314927e-02\n",
      " -7.41343386e-02 -1.05020185e-01  2.99892267e-02 -3.54317951e-03\n",
      " -2.22570185e-02 -2.12979530e-02 -9.33695092e-02  1.57476255e-02\n",
      " -1.85150853e-02  4.37493462e-02  8.00722636e-02 -2.61844250e-02\n",
      " -8.72720659e-02 -5.49214176e-02 -3.49022656e-02 -1.12987218e-01\n",
      " -6.78735568e-02 -7.21329165e-02 -6.25415229e-03 -4.09372421e-02\n",
      " -2.58413398e-02 -4.27247758e-02 -2.38944523e-02 -6.21252773e-02\n",
      " -1.33728573e-01  1.99910825e-02  3.40071399e-02 -1.87490159e-03\n",
      "  3.82246615e-03 -8.52025515e-02  3.89135034e-03 -5.22174560e-02\n",
      "  3.13369171e-02 -1.98501857e-02 -1.31563330e-02  1.00197536e-01\n",
      "  7.42238237e-02 -6.02828351e-02  8.65734892e-02 -5.93784287e-02\n",
      " -9.23654769e-02 -4.66553093e-02  5.32657012e-02 -8.51799579e-03\n",
      " -1.09049089e-01  1.27052364e-01  3.88237202e-02 -4.27085993e-02\n",
      " -3.65275008e-03 -7.20156194e-03  1.87714935e-02  1.92341748e-02\n",
      "  1.59178138e-02 -8.96496089e-02 -2.37940177e-02 -9.12860380e-02\n",
      " -2.20498112e-02  6.05272630e-02  2.54537326e-02 -2.58749500e-01]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = True\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "print(model['W1'].shape)\n",
    "print(model['W2'].shape)\n",
    "print(model['W2'])\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n",
      "resetting env. episode reward total was 0.000000. running mean: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-95d1cedb5477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mrmsprop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecay_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrmsprop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmsprop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mgrad_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset batch gradient buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "        for k,v in model.items():\n",
    "            g = grad_buffer[k] # gradient\n",
    "            rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "            model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "            grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "    #if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
